### Основные выводы

**Описание бейзлайна**
* Нашим бейзлайном будет пайплайн `TfidfVectorizer` + `OneVsRestClassifier(LogisticRegression)`, предсказание будем осуществлять только на основе объединенных в один признаков - текста статьи, её тегов и названия
* `TfidfVectorizer` преобразует текст в разреженную матрицу, а `OneVsRestClassifier` для каждой метки обучает отдельную линейную модель (`LogisticRegression`/`LinearSVC`), используя матрицу TF-IDF в качестве входных признаков
* Будем передавать в `GridSearchCV` различные параметры в пайплайн для нахождения наиболее оптимальных
* Для поиска лучших гиперпараметров внутри сетки будем использовать метрику `F1 с микро-усреднением`, поскольку она хорошо справляется с оценкой общей точности и полноты модели в задачах multilabel классификации на большом объеме данных с дисбалансом классов (наш случай)

**Предсказание хабов (тем) статей**
* Лучшей моделью для предсказания хабов (тем) статей оказалась `LinearSVC` (в связке с `OneVsRestClassifier`)
* Высокий Recall (71%-72%) указывает, что модель способна эффективно обнаруживать большинство истинных тем в статьях
* Низкий Hamming Loss (~2%) говорит о том, что в среднем модель делает мало ошибок на каждый пример, но это может быть обусловлено дисбалансом классов
* Относительно низкий Precision (38%-44%) показывает, что модель склонна к ложноположительным срабатываниям
* Модель имеет потенциал, но требует доработки, особенно в части повышения точности
![hubs](https://github.com/gelya01/hse-habr2024/blob/ML_baseline/ML_baseline/best_params/hubs.png)

**Предсказание рейтинга статей**
* Лучшей моделью для предсказания рейтинга статей оказалась `LogisticRegression` (в связке с `OneVsRestClassifier`)
* Метрика Hamming Loss (~50%) говорит о том, что модель правильно классифицирует половину случаев, что недостаточно для её практического применения
* Низкие макро-метрики по сравнению с взвешенными и микро-метриками указывают на то, что модель хуже справляется с редкими классами (очень низкие и низкие оценки)
* Одинаковые значения Precision, Recall и F1 (~50%) в микро-метриках указывают на сбалансированную производительность по основным классам, но ещё есть пространство для улучшений
* Текущая модель имеет ограниченную эффективность и требует доработки. Основные проблемы связаны с дисбалансом классов и неспособностью модели правильно классифицировать редкие оценки
![rating](https://github.com/gelya01/hse-habr2024/blob/ML_baseline/ML_baseline/best_params/rating.png)

**Что стоит улучшить/добавить на следующей итерации**
* Добавить остальные признаки (в текущей версии модель делает предсказание только на основе текста, названия и тегов статьи)
* Применить для задачи более сложные (нелинейные) модели
* Протестировать методы для балансировки классов (oversampling/undersampling и другие)
