{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Парсер статей Хабра Хабра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорты\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename='parser.log',\n",
    "    filemode='a',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "import requests\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from typing import List, Dict\n",
    "import aiohttp\n",
    "from fake_useragent import UserAgent, FakeUserAgentError\n",
    "pd.options.display.max_colwidth = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random user agent, куки и хэдэр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для создания случайного user_agent \n",
    "def get_random_user_agent() -> str:\n",
    "    \"\"\"\n",
    "    Возвращает случайный User-Agent для использования в HTTP-запросах\n",
    "\n",
    "    В случае ошибки при генерации случайного User-Agent возвращает стандартный User-Agent\n",
    "\n",
    "    Возвращает:\n",
    "        str: Случайный User-Agent или стандартный User-Agent (если произошла ошибка)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ua = UserAgent()\n",
    "        return ua.random\n",
    "    except FakeUserAgentError:\n",
    "        # Используем стандартный User-Agent в случае ошибки\n",
    "        return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "\n",
    "# Задаём дефолтные куки и заголовок\n",
    "cookies = {'hl': 'ru'}\n",
    "headers = {\n",
    "        'User-Agent': get_random_user_agent(),\n",
    "        'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсер хабов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для получения информации о хабах \n",
    "def parse_habr_hubs():\n",
    "    \"\"\"\n",
    "    Парсит информацию о хабах с Хабра Хабра и возвращает данные в виде DataFrame\n",
    "    Функция последовательно проходит страницы хабов, извлекая название, описание,\n",
    "    URL-адрес, рейтинг и количество подписчиков каждого хаба\n",
    "\n",
    "    Возвращает:\n",
    "        pd.DataFrame: Таблица с информацией о хабах, где каждая строка представляет один хаб\n",
    "\n",
    "    Столбцы в возвращаемом датафрейме:\n",
    "        - Название хаба (Title)\n",
    "        - Описание хаба (Description)\n",
    "        - URL-адрес хаба (URL)\n",
    "        - Рейтинг хаба (Rating)\n",
    "        - Количество подписчиков (Subscribers_cnt)\n",
    "    \"\"\"\n",
    "    page_number = 1\n",
    "    all_titles: List[str] = []\n",
    "    all_descriptions: List[str] = []\n",
    "    all_urls: List[str] = []\n",
    "    all_ratings: List[int] = []\n",
    "    all_subscribers: List[int] = []\n",
    "\n",
    "    while True:\n",
    "        url = f'https://habr.com/ru/hubs/page{page_number}/'\n",
    "        response = requests.get(url, headers=headers, cookies=cookies)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Ошибка при получении URL: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Проверяем, есть ли на странице хабы\n",
    "        hub_elements = soup.find_all('a', class_='tm-hub__title')\n",
    "        if not hub_elements:\n",
    "            print(\"Достигнут конец списка хабов\")\n",
    "            break  # Если хабов нет, выходим из цикла\n",
    "\n",
    "        # Названия хабов\n",
    "        titles = [title.get_text(strip=True) for title in hub_elements]\n",
    "        all_titles.extend(titles)\n",
    "\n",
    "        # Извлечение описания хабов\n",
    "        description_elements = soup.find_all('div', class_='tm-hub__description')\n",
    "        descriptions = [descr.get_text(strip=True) for descr in description_elements]\n",
    "        all_descriptions.extend(descriptions)\n",
    "\n",
    "        # Ссылки на хабы\n",
    "        urls = ['https://habr.com' + url['href'] for url in hub_elements]\n",
    "        all_urls.extend(urls)\n",
    "\n",
    "        # Рейтинги хабов\n",
    "        rating_elements = soup.find_all('div', class_='tm-hubs-list__hub-rating')\n",
    "        ratings = []\n",
    "        for rating in rating_elements:\n",
    "            rating_text = rating.get_text(strip=True).replace('Рейтинг','')\n",
    "            try:\n",
    "                ratings.append(int(float(rating_text)))\n",
    "            except ValueError:\n",
    "                ratings.append(None) \n",
    "        all_ratings.extend(ratings)\n",
    "\n",
    "        # Количество подписчиков хаба\n",
    "        subscriber_elements = soup.find_all('div', class_='tm-hubs-list__hub-subscribers')\n",
    "        subscribers = []\n",
    "        for subscriber in subscriber_elements:\n",
    "            text = subscriber.get_text(strip=True).replace('Подписчики','')\n",
    "            # Преобразуем количество подписчиков (в тысячах) в число\n",
    "            if 'K' in text:\n",
    "                subscribers.append(int(float(text.replace('K', '').replace(',', '.')) * 1000))\n",
    "            else:\n",
    "                try:\n",
    "                    subscribers.append(int(text))\n",
    "                except ValueError:\n",
    "                    subscribers.append(None)  \n",
    "        all_subscribers.extend(subscribers)\n",
    "\n",
    "        print(f\"Страница {page_number} обработана\")\n",
    "        page_number += 1\n",
    "        # Случайная задержка от 1 до 3 секунд, чтобы не словить блок от сервера\n",
    "        time.sleep(randint(1, 3))\n",
    "\n",
    "    # Создание DataFrame со всеми хабами\n",
    "    data = {\n",
    "        'Title': all_titles,\n",
    "        'Description': all_descriptions,\n",
    "        'URL': all_urls,\n",
    "        'Rating': all_ratings,\n",
    "        'Subscribers_cnt': all_subscribers\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubs = parse_habr_hubs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавление количества страниц внутри хабов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ограничение на 20 одновременных запросов через Semaphore\n",
    "semaphore = asyncio.Semaphore(20)\n",
    "\n",
    "# Асинхронная функция для получения количества страниц внутри хабов\n",
    "async def get_hub_pages_count_async(hub_url: str, session: aiohttp.ClientSession) -> int:\n",
    "    \"\"\"\n",
    "    Асинхронно получает количество страниц внутри хаба по URL\n",
    "\n",
    "    Параметры:\n",
    "        hub_url (str): URL-адрес хаба, откуда нужно получить количество страниц\n",
    "        session (aiohttp.ClientSession): Сессия для выполнения асинхронного запроса\n",
    "\n",
    "    Возвращает:\n",
    "        int: Общее количество страниц в хабе или None (если произошла ошибка)\n",
    "    \"\"\"\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(hub_url, cookies=cookies) as response:\n",
    "                if response.status != 200:\n",
    "                    print(f\"Ошибка при получении страницы хаба {hub_url}: {response.status}\")\n",
    "                    return None\n",
    "                content = await response.text()\n",
    "                soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "                # Находим блок пагинации\n",
    "                pagination = soup.find('div', class_='tm-pagination')\n",
    "                if pagination:\n",
    "                    pages = pagination.find_all('a', class_='tm-pagination__page')\n",
    "                    if pages:\n",
    "                        last_page = pages[-1].get_text(strip=True)\n",
    "                        try:\n",
    "                            total_pages = int(last_page)\n",
    "                        except ValueError:\n",
    "                            total_pages = 1\n",
    "                    else:\n",
    "                        total_pages = 1\n",
    "                else:\n",
    "                    total_pages = 1\n",
    "                return total_pages\n",
    "            \n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Исключение при получении {hub_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Функция для сбора результатов со всех страниц\n",
    "async def process_urls(urls: List[str]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Асинхронно обрабатывает список URL, извлекая количество страниц для каждого хаба\n",
    "\n",
    "    Параметры:\n",
    "        urls (List[str]): Список URL-адресов хабов для обработки\n",
    "\n",
    "    Возвращает:\n",
    "        List[int]: Список, содержащий количество страниц для каждого хаба или None (если произошла ошибка)\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession(headers=headers) as session:\n",
    "        tasks = [get_hub_pages_count_async(url, session) for url in urls]\n",
    "        results = await tqdm_asyncio.gather(*tasks, desc='Получаем количество страниц в хабах')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_lst = hubs['URL'].tolist() # список всех url\n",
    "res = asyncio.run(process_urls(url_lst)) # получаем количество страниц\n",
    "hubs.insert(5, 'Pages_cnt', res) # добавляем количество страниц в датафрейм хабов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cохраняем датафрейм со ссылками на хабы\n",
    "hubs.to_excel('hubs_urls.xlsx', index_label='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсер ссылок на статьи внутри хабов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.DataFrame(columns=['Title', 'URL', 'Hub']) # Создание итогового DataFrame\n",
    "semaphore = asyncio.Semaphore(20)  # Максимум 20 одновременных запросов\n",
    "\n",
    "# Асинхронная функция получения HTML-контента\n",
    "async def fetch(session: aiohttp.ClientSession, url: str) -> str:\n",
    "    \"\"\"\n",
    "    Асинхронно выполняет запрос для получения HTML-контента страницы по URL\n",
    "\n",
    "    Параметры:\n",
    "        session (aiohttp.ClientSession): Асинхронная сессия для выполнения HTTP-запроса\n",
    "        url (str): URL-адрес страницы, которую хотим загрузить\n",
    "\n",
    "    Возвращает:\n",
    "        str: Текстовый контент страницы или None (в случае ошибки)\n",
    "    \"\"\"\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(url, headers=headers, cookies=cookies, timeout=10) as response:\n",
    "                if response.status == 200:\n",
    "                    content = await response.text()\n",
    "                    await asyncio.sleep(randint(1, 3))\n",
    "                    return content\n",
    "                else:\n",
    "                    print(f\"Ошибка при получении {url}: {response.status}\")\n",
    "                    await asyncio.sleep(randint(1, 3))\n",
    "                    return None\n",
    "        except Exception as e:\n",
    "            print(f\"Исключение при получении {url}: {e}\")\n",
    "            await asyncio.sleep(randint(1, 3))\n",
    "            return None\n",
    "\n",
    "# Асинхронная функция парсинга статьи из HTML контента\n",
    "async def parse_articles_from_content(content: str, hub_name: str, df_full: pd.DataFrame) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Парсит статьи из HTML-контента\n",
    "\n",
    "    Параметры:\n",
    "        content (str): HTML-контент страницы\n",
    "        hub_name (str): Название хаба, к которому относятся статьи\n",
    "        df_full (pd.DataFrame): DataFrame, содержащий собранные статьи (для исключения дублей)\n",
    "\n",
    "    Возвращает:\n",
    "        List[Dict[str, str]]: Список словарей с информацией о статьях (заголовок, URL и хаб)\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    articles = []\n",
    "\n",
    "    article_elements = soup.find_all('a', class_='tm-title__link')\n",
    "    for article in article_elements:\n",
    "        title = article.get_text(strip=True)\n",
    "        link = 'https://habr.com' + article['href']\n",
    "        if len(df_full[df_full['URL']==link]) < 1: #условие, чтобы исключить дубли статей\n",
    "            articles.append({'Title': title, 'URL': link, 'Hub': hub_name})\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Асинхронная функция парсинга статей в хабе\n",
    "async def parse_habr_articles_in_hub(hub_url: str, df_full: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Асинхронно парсит статьи из хаба, извлекая все страницы и ссылки на статьи\n",
    "\n",
    "    Параметры:\n",
    "        hub_url (str): URL-адрес хаба, из которого извлекаются статьи\n",
    "        df_full (pd.DataFrame): DataFrame, содержащий собранные статьи (для исключения дублей)\n",
    "\n",
    "    Возвращает:\n",
    "        pd.DataFrame: DataFrame со статьями из хаба, где есть название статьи, URL и название хаба\n",
    "    \"\"\"\n",
    "    all_articles = []\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # Получение главной страницы хаба\n",
    "        response = await fetch(session, hub_url)\n",
    "        if not response:\n",
    "            print(f\"Не удалось получить главную страницу хаба: {hub_url}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "        # Извлечение названия хаба\n",
    "        hub_name_element = soup.find('h1', class_='tm-hub-card__name tm-hub-card__name_variant-base tm-hub-card__name')\n",
    "        if hub_name_element:\n",
    "            hub_name = hub_name_element.get_text(strip=True)\n",
    "        else:\n",
    "            hub_name = 'Unknown'\n",
    "\n",
    "        print(f\"Парсинг хаба: {hub_name}\")\n",
    "\n",
    "        # Находим общее количество страниц\n",
    "        pagination = soup.find('div', class_='tm-pagination')\n",
    "        if pagination:\n",
    "            pages = pagination.find_all('a', class_='tm-pagination__page')\n",
    "            if pages:\n",
    "                last_page = pages[-1].get_text(strip=True)\n",
    "                try:\n",
    "                    total_pages = int(last_page)\n",
    "                except ValueError:\n",
    "                    total_pages = 1\n",
    "            else:\n",
    "                total_pages = 1\n",
    "        else:\n",
    "            total_pages = 1\n",
    "\n",
    "        # Создаем задачи для всех страниц\n",
    "        tasks = []\n",
    "        for page in range(1, total_pages + 1):\n",
    "            url = f\"{hub_url}page{page}/\"\n",
    "            tasks.append(fetch(session, url))\n",
    "\n",
    "        # Запускаем задачи и собираем результаты\n",
    "        responses = await tqdm_asyncio.gather(*tasks, desc='Загрузка страниц')\n",
    "\n",
    "        # Парсим контента каждой страницы\n",
    "        for content in responses:\n",
    "            if content:\n",
    "                articles = await parse_articles_from_content(content, hub_name, df_full)\n",
    "                all_articles.extend(articles)\n",
    "\n",
    "    # Создание DataFrame со всеми ссылками на статьи\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск асинхронной функции\n",
    "for i in range(len(hubs)):\n",
    "    hub_url = hubs.iloc[i]['URL'] + 'articles/'\n",
    "    df = asyncio.run(parse_habr_articles_in_hub(hub_url, df_full))\n",
    "    df_full = pd.concat([df_full, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_full is not None:\n",
    "    print(f\"Всего собрано статей: {len(df_full)}\")\n",
    "    display(df_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Убираем дубликаты (если они где-то пробрались)\n",
    "display(df_full['URL'].duplicated().sum())\n",
    "hubs_full = df_full.drop_duplicates(subset='URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubs_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем итоговый датафрейм со ссылками на все статьи\n",
    "hubs_full.to_parquet('hubs_to_articles_urls.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсер статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Асинхронный парсер статей\n",
    "async def parse_habr_article(\n",
    "    url: str, \n",
    "    session: aiohttp.ClientSession, \n",
    "    semaphore: asyncio.Semaphore, \n",
    "    counter: List[int], \n",
    "    lock: asyncio.Lock\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Асинхронно парсит статью с Хабра по URL, извлекая данные страницы\n",
    "\n",
    "    Параметры:\n",
    "        url (str): URL-адрес статьи для парсинга\n",
    "        session (aiohttp.ClientSession): Асинхронная сессия для выполнения HTTP-запроса\n",
    "        semaphore (asyncio.Semaphore): Ограничение на одновременные запросы\n",
    "        counter (List[int]): Счётчик обработанных статей\n",
    "        lock (asyncio.Lock): Блокировка для безопасного обновления счётчика\n",
    "\n",
    "    Возвращает:\n",
    "        pd.DataFrame: DataFrame с информацией о статье (заголовок, автор, хабы и т.д.) или None в случае ошибки\n",
    "    \"\"\"\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                if response.status == 200:\n",
    "                    content = await response.text()\n",
    "                    await asyncio.sleep(randint(1, 3))\n",
    "                else:\n",
    "                    logging.warning(f\"Ошибка {response.status} при получении URL: {url}\")\n",
    "                    await asyncio.sleep(randint(1, 3))\n",
    "                    return None\n",
    "        except Exception as e:\n",
    "            logging.exception(f\"Исключение при получении {url}: {e}\")\n",
    "            await asyncio.sleep(randint(1, 3))\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Название статьи\n",
    "        title_text = soup.find('h1', class_='tm-title tm-title_h1')\n",
    "        title = title_text.get_text(strip=True) if title_text else None\n",
    "\n",
    "        # Имя автора\n",
    "        author_text = soup.find('a', class_='tm-user-info__username')\n",
    "        author = author_text.get_text(strip=True) if author_text else None\n",
    "\n",
    "        # Дата публикации\n",
    "        date_text = soup.find('time')\n",
    "        pub_date = pd.to_datetime(date_text['datetime']) if date_text else None\n",
    "\n",
    "        # Хабы\n",
    "        hub_elements = soup.find_all('a', class_='tm-hubs-list__link')\n",
    "        hubs = [hub.get_text(strip=True) for hub in hub_elements]\n",
    "\n",
    "        # Статья компании или физ. лица\n",
    "        individual_or_company = 'company' if any('Блог компании' in x for x in hubs) else 'individual'\n",
    "\n",
    "        # Теги\n",
    "        tag_elements = soup.find_all('a', class_='tm-tags-list__link')\n",
    "        tags = [tag.get_text(strip=True) for tag in tag_elements]\n",
    "\n",
    "        # Содержимое статьи\n",
    "        content_text = soup.find('div', class_='tm-article-body')\n",
    "        content = content_text.get_text(separator='\\n', strip=True) if content_text else None\n",
    "\n",
    "        # Количество комментариев\n",
    "        comments_text = soup.find('span', class_='tm-article-comments-counter-link__value')\n",
    "        comments = int(comments_text.get_text(strip=True) if comments_text else 0)\n",
    "\n",
    "        # Количество просмотров\n",
    "        views_text_ = soup.find('span', class_='tm-icon-counter__value')\n",
    "        views_text = views_text_.get_text(strip=True) if views_text_ else '0'\n",
    "        try:\n",
    "            if 'K' in views_text:\n",
    "                views = int(float(views_text.replace('K', '').replace(',', '.')) * 1000)\n",
    "            else:\n",
    "                views = int(views_text)\n",
    "        except ValueError:\n",
    "            views = -1  \n",
    "\n",
    "        # Время прочтения в минутах\n",
    "        reading_time_text = soup.find('span', class_='tm-article-reading-time__label')\n",
    "        reading_time = int(reading_time_text.get_text(strip=True).split()[0]) if reading_time_text else None\n",
    "\n",
    "        # Количество добавлений в закладки\n",
    "        bookmarks_text = soup.find('span', class_='bookmarks-button__counter')\n",
    "        bookmark = int(bookmarks_text.get_text(strip=True)) if bookmarks_text else None\n",
    "\n",
    "        # Ссылки на картинки\n",
    "        images = content_text.find_all(\"img\") if content_text else []\n",
    "        images_links = [img['src'] for img in images if img.has_attr('src')]\n",
    "\n",
    "        # Рейтинг статьи\n",
    "        article_rating_tag = soup.find('span', class_='tm-votes-meter__value')\n",
    "        if article_rating_tag:\n",
    "            article_rating = article_rating_tag.get_text(strip=True)\n",
    "        else:\n",
    "            article_rating_tag = soup.find('span', class_='tm-votes-lever__score-counter')\n",
    "            article_rating = article_rating_tag.get_text(strip=True) if article_rating_tag else '0'\n",
    "\n",
    "        # Позитивный или негативный рейтинг статьи\n",
    "        article_rating = article_rating or '0'\n",
    "        positive_negative = 'negative' if '-' in article_rating else 'positive'\n",
    "        article_rating_value = int(float(article_rating.replace('+', '').replace('-', ''))) if article_rating else 0\n",
    "\n",
    "        # Создание итогового DataFrame со статьями\n",
    "        data = {\n",
    "            'Title': [title],\n",
    "            'Author': [author],\n",
    "            'Publication_date': [pub_date],\n",
    "            'Hubs': [', '.join(hubs)],\n",
    "            'Tags': [', '.join(tags)],\n",
    "            'Content': [content],\n",
    "            'Comments': [comments],\n",
    "            'Views': [views],\n",
    "            'URL': [url],\n",
    "            'Reading_time': [reading_time],\n",
    "            'Images_links': [', '.join(images_links)],\n",
    "            'Individ/Company': [individual_or_company],\n",
    "            'Rating': [article_rating_value],\n",
    "            'Positive/Negative': [positive_negative],\n",
    "            'Bookmarks_cnt': [bookmark]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "    # Безопасное обновление счётчика из разных корутин\n",
    "        async with lock:\n",
    "            counter[0] += 1\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Ошибка при парсинге страницы {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для разделения данных на n частей с равномерным распределением элементов\n",
    "def split_list(lst: List, n: int) -> List[List]:\n",
    "    \"\"\"\n",
    "    Разделяет список на n подсписков с равномерным распределением элементов внутри\n",
    "\n",
    "    Параметры:\n",
    "        lst (List): Исходный список для разделения\n",
    "        n (int): Количество подсписков для разделения\n",
    "\n",
    "    Возвращает:\n",
    "        List[List]: Список из n подсписков, содержащих элементы исходного списка\n",
    "    \"\"\"\n",
    "    k, m = divmod(len(lst), n)\n",
    "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n",
    "\n",
    "# Разобъём на 5 частей (для более лёгкой обработки общего массива данных)\n",
    "urls = hubs_full['URL']\n",
    "hubs_parts = split_list(urls, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Асинхронная функция получения статей\n",
    "async def parse_article(\n",
    "    urls: List[str], \n",
    "    counter: List[int], \n",
    "    lock: asyncio.Lock, \n",
    "    semaphore_num: int = 20\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Асинхронно получает и парсит статьи по списку URL\n",
    "\n",
    "    Параметры:\n",
    "        urls (List[str]): Список URL-адресов статей для парсинга\n",
    "        counter (List[int]): Счётчик обработанных статей\n",
    "        lock (asyncio.Lock): Блокировка для безопасного обновления счётчика\n",
    "        semaphore_num (int): Максимальное количество одновременных запросов (по умолчанию 20)\n",
    "\n",
    "    Возвращает:\n",
    "        pd.DataFrame: DataFrame, содержащий данные по всем успешным парсингам статей\n",
    "    \"\"\"\n",
    "    semaphore = asyncio.Semaphore(semaphore_num)  # Ограничение до 20 одновременных запросов(чтобы получить все статьи на странице)\n",
    "    async with aiohttp.ClientSession(headers=headers, cookies=cookies) as session:\n",
    "        tasks = [parse_habr_article(url, session, semaphore, counter, lock) for url in urls]\n",
    "        results = []\n",
    "        for future in tqdm_asyncio.as_completed(tasks, total=len(tasks)):\n",
    "            result = await future\n",
    "            results.append(result)\n",
    "        dfs = [df for df in results if df is not None]\n",
    "        final_df = pd.concat(dfs, ignore_index=True)\n",
    "        # Выводим количество обработанных статей\n",
    "        print(f\"Обработано {len(final_df)} статей\")\n",
    "        return final_df\n",
    "\n",
    "# Асинхронная обработка частей массива со статьями (hubs_parts)\n",
    "async def process_part(\n",
    "    urls_chunk: List[str], \n",
    "    part_number: int, \n",
    "    counter: List[int], \n",
    "    lock: asyncio.Lock\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Асинхронно обрабатывает часть URL для парсинга статей, сохраняет результаты в файл\n",
    "\n",
    "    Параметры:\n",
    "        urls_chunk (List[str]): Часть списка URL-адресов для парсинга\n",
    "        part_number (int): Номер текущей части (используется для имени файла)\n",
    "        counter (List[int]): Счётчик обработанных статей\n",
    "        lock (asyncio.Lock): Блокировка для безопасного обновления счётчика\n",
    "\n",
    "    Возвращает:\n",
    "        None: Функция сохраняет результат в файл и ничего не возвращает\n",
    "    \"\"\"\n",
    "    # Ограничение до 50 одновременных запросов (для ускорения процесса берём ограничение больше, потерянные статьи обработаем отдельно)\n",
    "    final_df = await parse_article(urls_chunk, counter, lock, semaphore_num=50) \n",
    "    if final_df is not None:\n",
    "        filename = f'articles_part_{part_number}.parquet'\n",
    "        final_df.to_parquet(filename, index=False)\n",
    "        print(f\"Часть {part_number} сохранена в файл {filename}\")\n",
    "    else:\n",
    "        print(f\"Нет данных для сохранения в части {part_number}\")\n",
    "\n",
    "# Запускаем общий счётчик и блокировку\n",
    "global_counter = [0]\n",
    "global_lock = asyncio.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем функцию обработки статей\n",
    "for i, urls_chunk in enumerate(hubs_parts, 1):\n",
    "    print(f\"Начинается обработка части {i} из {len(hubs_parts)}\")\n",
    "    asyncio.run(process_part(urls_chunk, i, global_counter, global_lock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём итоговый датафрейм из 5 отдельных файлов\n",
    "fin_df = pd.DataFrame()\n",
    "for part in range(1, len(hubs_parts)+1):\n",
    "    df_part = pd.read_parquet(f'articles_part_{part}.parquet')\n",
    "    fin_df = pd.concat([fin_df, df_part], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Находим потерянные при получении статьи (где был Semaphore = 50) URL, сохраняем их и добавляем в итоговый датафрейм\n",
    "url_dif = list(set(urls) - set(fin_df['URL']))\n",
    "missed_articles = asyncio.run(parse_article(url_dif, global_counter, global_lock))\n",
    "missed_articles.to_parquet('missed_articles.parquet', index=False)\n",
    "fin_df = pd.concat([fin_df, missed_articles], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение итогов\n",
    "fin_df.to_parquet('habr_articles_parsed_final.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
